{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAIST Scene Text Database 전처리기:\n",
    "1. 데이터 url:http://www.iapr-tc11.org/mediawiki/index.php/KAIST_Scene_Text_Database\n",
    "1. 압축을 다 풀고 'kaist_dataset'아래에 이미지 데이터와 xml 파일을 두면\n",
    "1. 각 xml 파일에서 글자하나하나 위치정보를 읽어 원본 이미지에서 글자를 별도의 이미지 파일로 저장하는 작업을 수행한다.\n",
    "1. 결과물은 char_data 아래에 각 글자명으로 폴더를 만들고 그 아래에 원본파일명.글자명.idx.jpg로 저장\n",
    "    - char_data/{character}/{img_filename}.{character}.{idx}.jpg\n",
    "    - character가 .(dot)인 경우에는 {character}값을 dot으로 대체하여 처리\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'가': 0,\n",
       " '각': 1,\n",
       " '간': 2,\n",
       " '갇': 3,\n",
       " '갈': 4,\n",
       " '갉': 5,\n",
       " '갊': 6,\n",
       " '감': 7,\n",
       " '갑': 8,\n",
       " '값': 9,\n",
       " '갓': 10,\n",
       " '갔': 11,\n",
       " '강': 12,\n",
       " '갖': 13,\n",
       " '갗': 14,\n",
       " '같': 15,\n",
       " '갚': 16,\n",
       " '갛': 17,\n",
       " '개': 18,\n",
       " '객': 19}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from os import listdir, path, makedirs\n",
    "from os.path import isfile, isdir, join, basename\n",
    "\n",
    "base_path = 'phd08_output'\n",
    "\n",
    "def read_charlist(base_path):\n",
    "    data_filenames = [path.splitext(f)[0] for f in listdir(base_path) if isfile(join(base_path, f)) and f.lower().endswith('.csv')]        \n",
    "    \n",
    "    return data_filenames\n",
    "        \n",
    "data_filenames = read_charlist(base_path)\n",
    "data_filenames = data_filenames[:20]\n",
    "\n",
    "label_idx = {x:idx for idx, x in enumerate(sorted(data_filenames))}\n",
    "\n",
    "label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('eval_dir', '/tmp/phd08_eval',\n",
    "                           \"\"\"Directory where to write event logs.\"\"\")\n",
    "tf.app.flags.DEFINE_string('eval_data', 'test',\n",
    "                           \"\"\"Either 'test' or 'train_eval'.\"\"\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', '/tmp/phd08_train',\n",
    "                           \"\"\"Directory where to read model checkpoints.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5,\n",
    "                            \"\"\"How often to run the eval.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_examples', 1000,\n",
    "                            \"\"\"Number of examples to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('run_once', False,\n",
    "                            \"\"\"Whether to run eval only once.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.batch_size = 128\n",
    "FLAGS.eval_interval_secs = 60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'batch:0' shape=(128, 56, 56, 3) dtype=float32>,\n",
       " <tf.Tensor 'Reshape_1:0' shape=(128,) dtype=int32>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    # Create a queue that produces the filenames to read.\n",
    "    filenames = [join(base_path, '.'.join([char, 'csv'])) for char in data_filenames]\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    line_reader = tf.TextLineReader()\n",
    "    # Read a whole file from the queue, the first returned value in the tuple is the\n",
    "    # filename which we are ignoring.\n",
    "    _, csv_row = line_reader.read(filename_queue)\n",
    "    \n",
    "    '''\n",
    "        Columns:\n",
    "        FO(1): Font type (B:바다, D:돋움, G:고딕, H1:한양해서, H2:헤드라인, M:명조, N:나무, S:샘물, Y:엽서)\n",
    "        FS(1): Font size (0:12, 1:13: 2:14)\n",
    "        CP(1): The number of copies (0:0, 1:1, 2:2)\n",
    "        RE(1): Resolution (0:200, 1:240, 2:280)\n",
    "        TH(1): Threshold (0:140, 1:180, 2:220)\n",
    "        SL(1): Slope(Rotate) (0:-3deg, 1:0deg, 2:3deg)\n",
    "        HE(1): Height(pixels)\n",
    "        WD(1): Width(pixels)\n",
    "        Korean Character(1) (가, 각, 간, ...)\n",
    "        Image data(The number of columns = Rows X Cols)\n",
    "        Label(1): (0:가, 1:각, 2:간 ...)\n",
    "\n",
    "    '''\n",
    "    record_defaults = [\n",
    "        [''], # FO\n",
    "        [''], # FS\n",
    "        [0],  # CP\n",
    "        [0],  # RE\n",
    "        [0],  # TH\n",
    "        [0],  # SL\n",
    "        [0],  # HE\n",
    "        [0],  # WD\n",
    "        [''],  # Kor char.\n",
    "    ] + [[0]] * (56*56) + [[0]]\n",
    "\n",
    "    columns = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    character = columns[8]\n",
    "    image = tf.reshape(tf.stack(columns[9:-1]), [56, 56, 1])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    label = columns[-1]\n",
    "    \n",
    "    '''\n",
    "    To check valid input\n",
    "    '''\n",
    "#     with tf.Session() as sess:\n",
    "#         coord = tf.train.Coordinator()\n",
    "#         threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#         fig = plt.figure(figsize=(40,250))\n",
    "#         for i in range(10):\n",
    "#             char_value, l_value, image_value = sess.run([character, label, image])\n",
    "#             print(str(char_value))\n",
    "#             print(l_value)\n",
    "#             print(image_value)\n",
    "#             subplot = fig.add_subplot(50,10,i+1)\n",
    "#             subplot.set_xlabel(l_value)\n",
    "#             plt.imshow(image_value)\n",
    "#         plt.show()            \n",
    "#         coord.request_stop()\n",
    "#         coord.join(threads)             \n",
    "\n",
    "#     print('image', image)\n",
    "#     print('label', label)\n",
    "    \n",
    "    num_preprocess_threads = 1\n",
    "    min_queue_examples = 256\n",
    "\n",
    "    images, label_batch = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * FLAGS.batch_size)\n",
    "\n",
    "#     tf.summary.image('images', images)\n",
    "#     tf.summary.histogram('label_batch', label_batch)\n",
    "    return images, tf.reshape(label_batch, [FLAGS.batch_size])\n",
    "\n",
    "get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(label_idx)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "TOWER_NAME = 'tower'\n",
    "NUM_CLASSES = len(label_idx)\n",
    "\n",
    "\n",
    "def _activation_summary(x):\n",
    "    \"\"\"Helper to create summaries for activations.\n",
    "\n",
    "    Creates a summary that provides a histogram of activations.\n",
    "    Creates a summary that measures the sparsity of activations.\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "    Returns:\n",
    "      nothing\n",
    "    \"\"\"\n",
    "    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "    # session. This helps the clarity of presentation on tensorboard.\n",
    "    tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                      tf.nn.zero_fraction(x))\n",
    "\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "\n",
    "    Args:\n",
    "      name: name of the variable\n",
    "      shape: list of ints\n",
    "      initializer: initializer for Variable\n",
    "\n",
    "    Returns:\n",
    "      Variable Tensor\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        dtype = tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "\n",
    "    Note that the Variable is initialized with a truncated normal distribution.\n",
    "    A weight decay is added only if one is specified.\n",
    "\n",
    "    Args:\n",
    "      name: name of the variable\n",
    "      shape: list of ints\n",
    "      stddev: standard deviation of a truncated Gaussian\n",
    "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "          decay is not added for this Variable.\n",
    "\n",
    "    Returns:\n",
    "      Variable Tensor\n",
    "    \"\"\"\n",
    "    dtype = tf.float32\n",
    "    var = _variable_on_cpu(\n",
    "        name,\n",
    "        shape,\n",
    "        tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "\n",
    "    Generates moving average for all losses and associated summaries for\n",
    "    visualizing the performance of the network.\n",
    "\n",
    "    Args:\n",
    "        total_loss: Total loss from loss().\n",
    "    Returns:\n",
    "        loss_averages_op: op for generating moving averages of losses.\n",
    "    \"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op\n",
    "\n",
    "def _train(total_loss, global_step):\n",
    "    \"\"\"Train CIFAR-10 model.\n",
    "\n",
    "    Create an optimizer and apply to all trainable variables. Add moving\n",
    "    average for all trainable variables.\n",
    "\n",
    "    Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    global_step: Integer Variable counting the number of training steps\n",
    "      processed.\n",
    "    Returns:\n",
    "    train_op: op for training.\n",
    "    \"\"\"\n",
    "    # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "      MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op\n",
    "\n",
    "def inference(images):\n",
    "    \"\"\"Build the CIFAR-10 model.\n",
    "\n",
    "    Args:\n",
    "      images: Images returned from distorted_inputs() or inputs().\n",
    "\n",
    "    Returns:\n",
    "      Logits.\n",
    "    \"\"\"\n",
    "    # We instantiate all variables using tf.get_variable() instead of\n",
    "    # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "    # If we only ran this model on a single GPU, we could simplify this function\n",
    "    # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "    #\n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[3, 3, 3, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1)\n",
    "        \n",
    "    # conv1-1\n",
    "    with tf.variable_scope('conv1-1') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[3, 3, 64, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(conv1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1_1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1_1)\n",
    "\n",
    "    # conv1-2\n",
    "    with tf.variable_scope('conv1-2') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[3, 3, 64, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1_2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1_2)\n",
    "    \n",
    "    \n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                      name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[3, 3, 64, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2)\n",
    "\n",
    "    # conv2-1\n",
    "    with tf.variable_scope('conv2-1') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[3, 3, 64, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(conv2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2_1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2_1)\n",
    "\n",
    "    # conv2-2\n",
    "    with tf.variable_scope('conv2-2') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[3, 3, 64, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2_2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2_2)\n",
    "        \n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2_2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                      name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                           strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        # Move everything into depth so we can perform a single matrix multiply.\n",
    "        reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                              stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "        _activation_summary(local3)\n",
    "\n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                              stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        _activation_summary(local4)\n",
    "    print('local3', local3)\n",
    "    print('local4', local4)\n",
    "\n",
    "    # linear layer(WX + b),\n",
    "    # We don't apply softmax here because\n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "    # and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "                                              stddev=1 / 192.0, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                                  tf.constant_initializer(0.0))\n",
    "        \n",
    "        print('weights', weights)\n",
    "        print('biases', biases)\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        \n",
    "        print('softmax_linear', softmax_linear)\n",
    "        _activation_summary(softmax_linear)\n",
    "\n",
    "    return softmax_linear\n",
    "\n",
    "def _loss(logits, labels):\n",
    "    \"\"\"Add L2Loss to all the trainable variables.\n",
    "\n",
    "    Add summary for \"Loss\" and \"Loss/avg\".\n",
    "    Args:\n",
    "      logits: Logits from inference().\n",
    "      labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "              of shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "      Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    # Calculate the average cross entropy loss across the batch.\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "    # decay terms (L2 loss).\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        images, labels = get_inputs()\n",
    "        \n",
    "        print('labels.get_shape()', labels.get_shape())\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        \n",
    "        # Build a Graph that computes the logits predictions from the\n",
    "        # inference model.\n",
    "        logits = inference(images)\n",
    "        \n",
    "        print('images, logits, labels', images, logits, labels)\n",
    "\n",
    "        # Calculate loss.\n",
    "        loss = _loss(logits, labels)\n",
    "\n",
    "        # Build a Graph that trains the model with one batch of examples and\n",
    "        # updates the model parameters.\n",
    "        train_op = _train(loss, global_step)\n",
    "\n",
    "        class _LoggerHook(tf.train.SessionRunHook):\n",
    "            \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "            def begin(self):\n",
    "                self._step = -1\n",
    "                self._start_time = time.time()\n",
    "\n",
    "            def before_run(self, run_context):\n",
    "                self._step += 1\n",
    "                return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "            def after_run(self, run_context, run_values):\n",
    "                if self._step % FLAGS.log_frequency == 0:\n",
    "                    current_time = time.time()\n",
    "                    duration = current_time - self._start_time\n",
    "                    self._start_time = current_time\n",
    "\n",
    "                    loss_value = run_values.results\n",
    "                    examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n",
    "                    sec_per_batch = float(duration / FLAGS.log_frequency)\n",
    "\n",
    "                    format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                                  'sec/batch)')\n",
    "                    print(format_str % (datetime.now(), self._step, loss_value,\n",
    "                                        examples_per_sec, sec_per_batch))\n",
    "\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "                checkpoint_dir=FLAGS.train_dir,\n",
    "                hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n",
    "                       tf.train.NanTensorHook(loss),\n",
    "                       _LoggerHook()],\n",
    "                config=tf.ConfigProto(\n",
    "                    log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n",
    "            while not mon_sess.should_stop():\n",
    "                mon_sess.run(train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local3 Tensor(\"local3/local3:0\", shape=(128, 384), dtype=float32)\n",
      "local4 Tensor(\"local4/local4:0\", shape=(128, 192), dtype=float32)\n",
      "weights <tf.Variable 'softmax_linear/weights:0' shape=(192, 20) dtype=float32_ref>\n",
      "biases <tf.Variable 'softmax_linear/biases:0' shape=(20,) dtype=float32_ref>\n",
      "softmax_linear Tensor(\"softmax_linear/softmax_linear:0\", shape=(128, 20), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from /tmp/phd08_train/model.ckpt-16886\n",
      "logit_val [[-0.00116221 -0.12091798 -0.04991271 ...,  0.25093821  0.08635263\n",
      "   0.07991093]\n",
      " [-0.00116221 -0.12091798 -0.04991271 ...,  0.25093821  0.08635263\n",
      "   0.07991093]\n",
      " [-0.00116221 -0.12091798 -0.04991271 ...,  0.25093821  0.08635263\n",
      "   0.07991093]\n",
      " ..., \n",
      " [-0.00116221 -0.12091798 -0.04991271 ...,  0.25093821  0.08635263\n",
      "   0.07991093]\n",
      " [-0.00116221 -0.12091798 -0.04991271 ...,  0.25093821  0.08635263\n",
      "   0.07991093]\n",
      " [-0.00116221 -0.12091798 -0.04991271 ...,  0.25093821  0.08635263\n",
      "   0.07991093]]\n",
      "pred_label, label_val, predictions [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17] [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11] [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "2017-09-29 11:14:01.386800: precision @ 1 = 0.000\n",
      "2017-09-29 11:14:01.386915: precision @ 5 = 0.000\n",
      "INFO:tensorflow:Restoring parameters from /tmp/phd08_train/model.ckpt-17784\n",
      "logit_val [[ 0.00912651 -0.03052503  0.03481415 ...,  0.11545173  0.03107884\n",
      "   0.08995777]\n",
      " [ 0.00912651 -0.03052503  0.03481415 ...,  0.11545173  0.03107884\n",
      "   0.08995777]\n",
      " [ 0.00912651 -0.03052503  0.03481415 ...,  0.11545173  0.03107884\n",
      "   0.08995777]\n",
      " ..., \n",
      " [ 0.00912651 -0.03052503  0.03481415 ...,  0.11545173  0.03107884\n",
      "   0.08995777]\n",
      " [ 0.00912651 -0.03052503  0.03481415 ...,  0.11545173  0.03107884\n",
      "   0.08995777]\n",
      " [ 0.00912651 -0.03052503  0.03481415 ...,  0.11545173  0.03107884\n",
      "   0.08995777]]\n",
      "pred_label, label_val, predictions [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6] [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "2017-09-29 12:14:37.244298: precision @ 1 = 0.000\n",
      "2017-09-29 12:14:37.244837: precision @ 5 = 1.000\n",
      "INFO:tensorflow:Restoring parameters from /tmp/phd08_train/model.ckpt-18461\n",
      "logit_val [[-0.02211365 -0.0099932  -0.03494459 ...,  0.10518486  0.07204047\n",
      "   0.09725344]\n",
      " [-0.02211365 -0.0099932  -0.03494459 ...,  0.10518486  0.07204047\n",
      "   0.09725344]\n",
      " [-0.02211365 -0.0099932  -0.03494459 ...,  0.10518486  0.07204047\n",
      "   0.09725344]\n",
      " ..., \n",
      " [-0.02211365 -0.0099932  -0.03494459 ...,  0.10518486  0.07204047\n",
      "   0.09725344]\n",
      " [-0.02211365 -0.0099932  -0.03494459 ...,  0.10518486  0.07204047\n",
      "   0.09725344]\n",
      " [-0.02211365 -0.0099932  -0.03494459 ...,  0.10518486  0.07204047\n",
      "   0.09725344]]\n",
      "pred_label, label_val, predictions [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17] [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6] [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "2017-09-29 13:15:15.243428: precision @ 1 = 0.000\n",
      "2017-09-29 13:15:15.243661: precision @ 5 = 0.000\n",
      "INFO:tensorflow:Restoring parameters from /tmp/phd08_train/model.ckpt-19132\n",
      "logit_val [[-0.05219613 -0.02722904 -0.027738   ...,  0.11793685 -0.00280066\n",
      "  -0.03474106]\n",
      " [-0.05219613 -0.02722904 -0.027738   ...,  0.11793685 -0.00280066\n",
      "  -0.03474106]\n",
      " [-0.05219613 -0.02722904 -0.027738   ...,  0.11793685 -0.00280066\n",
      "  -0.03474106]\n",
      " ..., \n",
      " [-0.05219613 -0.02722904 -0.027738   ...,  0.11793685 -0.00280066\n",
      "  -0.03474106]\n",
      " [-0.05219613 -0.02722904 -0.027738   ...,  0.11793685 -0.00280066\n",
      "  -0.03474106]\n",
      " [-0.05219613 -0.02722904 -0.027738   ...,  0.11793685 -0.00280066\n",
      "  -0.03474106]]\n",
      "pred_label, label_val, predictions [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17] [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 18 18 18] [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-29 14:15:51.775464: precision @ 1 = 0.000\n",
      "2017-09-29 14:15:51.775672: precision @ 5 = 0.000\n",
      "INFO:tensorflow:Restoring parameters from /tmp/phd08_train/model.ckpt-20033\n",
      "logit_val [[ 0.0713073   0.04679133 -0.04766726 ...,  0.08666516  0.06077418\n",
      "  -0.14177787]\n",
      " [ 0.0713073   0.04679133 -0.04766726 ...,  0.08666516  0.06077418\n",
      "  -0.14177787]\n",
      " [ 0.0713073   0.04679133 -0.04766726 ...,  0.08666516  0.06077418\n",
      "  -0.14177787]\n",
      " ..., \n",
      " [ 0.0713073   0.04679133 -0.04766726 ...,  0.08666516  0.06077418\n",
      "  -0.14177787]\n",
      " [ 0.0713073   0.04679133 -0.04766726 ...,  0.08666516  0.06077418\n",
      "  -0.14177787]\n",
      " [ 0.0713073   0.04679133 -0.04766726 ...,  0.08666516  0.06077418\n",
      "  -0.14177787]]\n",
      "pred_label, label_val, predictions [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17] [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6] [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "2017-09-29 15:16:19.420173: precision @ 1 = 0.000\n",
      "2017-09-29 15:16:19.420387: precision @ 5 = 0.000\n",
      "INFO:tensorflow:Restoring parameters from /tmp/phd08_train/model.ckpt-21081\n",
      "logit_val [[-0.02649466  0.01872155 -0.04374643 ...,  0.08396687  0.04346378\n",
      "  -0.04425978]\n",
      " [-0.02649466  0.01872155 -0.04374643 ...,  0.08396687  0.04346378\n",
      "  -0.04425978]\n",
      " [-0.02649466  0.01872155 -0.04374643 ...,  0.08396687  0.04346378\n",
      "  -0.04425978]\n",
      " ..., \n",
      " [-0.02649466  0.01872155 -0.04374643 ...,  0.08396687  0.04346378\n",
      "  -0.04425978]\n",
      " [-0.02649466  0.01872155 -0.04374643 ...,  0.08396687  0.04346378\n",
      "  -0.04425978]\n",
      " [-0.02649466  0.01872155 -0.04374643 ...,  0.08396687  0.04346378\n",
      "  -0.04425978]]\n",
      "pred_label, label_val, predictions [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4] [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "2017-09-29 16:16:44.482003: precision @ 1 = 0.000\n",
      "2017-09-29 16:16:44.482123: precision @ 5 = 0.000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "TOWER_NAME = 'tower'\n",
    "NUM_CLASSES = len(label_idx)\n",
    "\n",
    "\n",
    "def eval_once(saver, summary_writer, top_k_op, top_5_op, summary_op, logits, labels):\n",
    "    \"\"\"Run Eval once.\n",
    "\n",
    "    Args:\n",
    "      saver: Saver.\n",
    "      summary_writer: Summary writer.\n",
    "      top_k_op: Top K op.\n",
    "      summary_op: Summary op.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            # Restores from checkpoint\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            # Assuming model_checkpoint_path looks something like:\n",
    "            #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "            # extract global_step from it.\n",
    "            global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "            return\n",
    "\n",
    "        # Start the queue runners.\n",
    "        coord = tf.train.Coordinator()\n",
    "        try:\n",
    "            threads = []\n",
    "            for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "                threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "                                                 start=True))\n",
    "\n",
    "            num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n",
    "            true_count = 0  # Counts the number of correct predictions.\n",
    "            true_count_top5 = 0  # Counts the number of correct predictions.\n",
    "            total_sample_count = num_iter * FLAGS.batch_size\n",
    "            step = 0\n",
    "            while step < num_iter and not coord.should_stop():\n",
    "                predictions, predictions_top5, logit_val, label_val = sess.run([top_k_op, top_5_op, logits, labels])\n",
    "                \n",
    "                if step % 1000 == 0:\n",
    "                    import numpy as np\n",
    "                    \n",
    "                    pred_label = [np.argmax(logit) for logit in logit_val]\n",
    "                    print('logit_val', logit_val)\n",
    "                    print('pred_label, label_val, predictions', pred_label, label_val, predictions)\n",
    "                    \n",
    "#                 pred_label = [np.argmax(logit) for logit in logit_val]\n",
    "#                 print('logit_val', logit_val)\n",
    "#                 print('pred_label, label_val, predictions', pred_label, label_val, predictions)\n",
    "                true_count += np.sum(predictions)\n",
    "                true_count_top5 += np.sum(predictions_top5)\n",
    "                step += 1\n",
    "\n",
    "            # Compute precision @ 1.\n",
    "            precision = true_count / total_sample_count\n",
    "            precision_top5 = true_count_top5 / total_sample_count\n",
    "            print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "            print('%s: precision @ 5 = %.3f' % (datetime.now(), precision_top5))\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.ParseFromString(sess.run(summary_op))\n",
    "            summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "            summary.value.add(tag='Precision @ 5', simple_value=precision_top5)\n",
    "            summary_writer.add_summary(summary, global_step)\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            coord.request_stop(e)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads, stop_grace_period_secs=10)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    \"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # Get images and labels for CIFAR-10.\n",
    "#         eval_data = FLAGS.eval_data == 'test'\n",
    "#         images, labels = cifar10.inputs(eval_data=eval_data)\n",
    "        images, labels = get_inputs()\n",
    "\n",
    "        # Build a Graph that computes the logits predictions from the\n",
    "        # inference model.\n",
    "        logits = inference(images)\n",
    "        \n",
    "        # Calculate predictions.\n",
    "        top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "        top_5_op = tf.nn.in_top_k(logits, labels, 5)\n",
    "\n",
    "        # Restore the moving average version of the learned variables for eval.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(\n",
    "            MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        # Build the summary operation based on the TF collection of Summaries.\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n",
    "        \n",
    "        while True:\n",
    "            eval_once(saver, summary_writer, top_k_op, top_5_op, summary_op, logits, labels)\n",
    "            if FLAGS.run_once:\n",
    "                break\n",
    "            time.sleep(FLAGS.eval_interval_secs)\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
